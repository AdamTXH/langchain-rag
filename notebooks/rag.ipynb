{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to root directory\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest into vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model=thenlper/gte-large\n",
    "volume=/data/llm/hf-tei-data\n",
    "docker images run --gpus all --env HTTPS_PROXY=$https_proxy --env HTTP_PROXY=$http_proxy -p 8188:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:turing-0.6 --model-id $model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceHubEmbeddings\n",
    "embeddings_model = HuggingFaceHubEmbeddings(model=\"http://localhost:8188\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, chunk, store source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"\") # Add the path to the .pdf document here\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(pages)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings_model, persist_directory=\"database/chromadb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vecstore = Chroma(persist_directory=\"database/chromadb\", embedding_function=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "retrieved_docs = retriever.invoke(\"Who are the new hires?\")\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    "    azure_deployment=\"llm-rag-chatgpt35\",\n",
    "    max_tokens=2048,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt template\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"Who are the new hires\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "from rag.generate import *\n",
    "from rag.ingest import *\n",
    "from rag.retrieve import *\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query\n",
    "query = \"Who are the new hires?\"\n",
    "chromadb_save_path = \"database/chromadb/flex_neo\"\n",
    "\n",
    "# Ingest\n",
    "print(\"Ingest\")\n",
    "embedding_model = get_embedding_model(\"http://localhost:8188\")\n",
    "documents = load_documents(\"\") # Add the path to the .pdf document here\n",
    "chunks = chunk_documents(doc= documents, chunk_size=2000, chunk_overlap=0)\n",
    "save_chunks_to_chroma(chunks, embedding_model, chromadb_save_path)\n",
    "\n",
    "# Retriever\n",
    "print(\"\\nRetrieval\")\n",
    "retriever = get_chroma_retriever(chromadb_save_path, embedding_model)\n",
    "contexts = retrieve(retriever, query)\n",
    "\n",
    "# Rag Chain\n",
    "print(\"\\nGeneration\")\n",
    "llm = get_llm_azure(deployment_name = \"llm-rag-chatgpt35\", max_tokens=2048, temperature=0.7)\n",
    "prompt_template = get_prompt_template() # Use the default template by not giving input args\n",
    "rag_chain = create_rag_chain(retriever, prompt_template, llm)\n",
    "\n",
    "# Chat\n",
    "import time\n",
    "t1 = time.time()\n",
    "generated = rag_chain.invoke(query)\n",
    "t2 = time.time() -t1\n",
    "word_count = len(generated.split())\n",
    "print(f\"Generation took {(t2)*1000:.4f} ms. {word_count} words. {word_count / t2:.4f} words per second.\\n\")\n",
    "\n",
    "print(\"Results:\")\n",
    "print(f\" Query:\\n\\t{query}\")\n",
    "print(f\" Rag generation:\\n\\t{textwrap.fill(generated, width=100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming\n",
    "async for chunk in rag_chain.astream(query):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await stream_output(rag_chain, query)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What does intel flex engineering do?\"\n",
    "print(\" Response without RAG:\\n\")\n",
    "print(textwrap.fill(llm.invoke(query).content, width=100))\n",
    "print()\n",
    "print(\" Response with RAG:\\n\")\n",
    "print(textwrap.fill(rag_chain.invoke(query)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral 7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.generate import *\n",
    "from rag.ingest import *\n",
    "from rag.retrieve import *\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query\n",
    "query = \"Who are the new hires?\"\n",
    "chromadb_save_path = \"database/chromadb/flex_neo\"\n",
    "\n",
    "# Ingest\n",
    "print(\"Ingest\")\n",
    "embedding_model = get_embedding_model(\"http://localhost:8188\")\n",
    "documents = load_documents(\"\") # Add the path to the document here\n",
    "chunks = chunk_documents(doc= documents, chunk_size=2000, chunk_overlap=0)\n",
    "save_chunks_to_chroma(chunks, embedding_model, chromadb_save_path)\n",
    "\n",
    "# Retriever\n",
    "print(\"\\nRetrieval\")\n",
    "retriever = get_chroma_retriever(chromadb_save_path, embedding_model)\n",
    "contexts = retrieve(retriever, query)\n",
    "\n",
    "# Rag Chain \n",
    "print(\"\\nGeneration\")\n",
    "mistral_llm = get_llm_llamacpp(\"/data/llm/models/mistral-7b-instruct-v0.2.FP16.gguf\")\n",
    "prompt_template = get_prompt_template() # Use the default template by not giving input args\n",
    "rag_chain = create_rag_chain(retriever, prompt_template, mistral_llm)\n",
    "\n",
    "# Chat\n",
    "import time\n",
    "t1 = time.time()\n",
    "generated = rag_chain.invoke(query)\n",
    "t2 = time.time() -t1\n",
    "word_count = len(generated.split())\n",
    "print(f\"Generation took {(t2)*1000:.4f} ms. {word_count} words. {word_count / t2:.4f} words per second.\\n\")\n",
    "\n",
    "print(\"Results:\")\n",
    "print(f\" Query:\\n\\t{query}\")\n",
    "print(f\" Rag generation:\\n\\t{textwrap.fill(generated, width=100)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computex",
   "language": "python",
   "name": "computex"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
