{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest into vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model=thenlper/gte-large\n",
    "volume=/data/llm/hf-tei-data\n",
    "docker images run --gpus all --env HTTPS_PROXY=$https_proxy --env HTTP_PROXY=$http_proxy -p 8188:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:turing-0.6 --model-id $model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceHubEmbeddings\n",
    "embeddings_model = HuggingFaceHubEmbeddings(model=\"http://localhost:8188\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, chunk, store source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"documents/2024 Intel Flex Engineering Malaysia - NEO_20240228.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(pages)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings_model, persist_directory=\"database/chromadb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vecstore = Chroma(persist_directory=\"database/chromadb\", embedding_function=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Intel Confidential Intel Flex\\n 2Intel Flex Malaysia Engineering & TFM\\nRecent New Hires\\nEngineering TFM\\nTan, An Nie\\nMohamad, Siti Nurhanisah\\n(Hanisah)\\nTai, Andre\\nWei Xiang\\nEe, Elgene  \\nDing RenNah, Wan Jun\\n(Nicole )\\nTay, Xue Hao\\n(Adam )Ong, Frankie\\nWei Quan', metadata={'page': 1, 'source': 'documents/2024 Intel Flex Engineering Malaysia - NEO_20240228.pdf'}),\n",
       " Document(page_content='Intel Confidential Intel Flex\\n 13Experienced employee assigned to \\nbefriend the new hire in order to help \\nintegrating the new hire to Intel and \\nIntel Flex.\\nHello, buddy!\\nQuarterly Group Buddy Meetup1:1 Meeting\\n6 months\\nBuddy Check List\\n Buddy Lunch\\nLearn More: Intel Flex Malaysia - New Hire Integration Program', metadata={'page': 12, 'source': 'documents/2024 Intel Flex Engineering Malaysia - NEO_20240228.pdf'}),\n",
       " Document(page_content='Intel Confidential Intel Flex\\n 16\\nEach new hire will receive multiple ramp -up list, incl. Focused List \\nfrom Mini -TFA, Modern SW, potentially project -specific and Flex \\nGlobal passdownMultiple Ramp -Up Lists (Intensive Academy, Mini -TFA, MSW…)Buddy Program initiatedNew Hire Joins\\n1. Project Lead, Mini -TFAL and Technical Buddy work together \\nto prioritize for the resource\\n2. Identify the completion date of the ramp -upRamp -Up Personalization with “Involved Personnel”\\n1. Project specific, skills ramp -up & task execution likely go hand -\\nin-hand together.\\n2. Mini -TFAL provides the fundamental exposure to the field \\nbeyond “project -specific”Skills Practice\\nRecommended to have weekly progress with 2 parts, project \\nspecific tasks (if projects assigned) and personalized list from step \\n#2Regular Update\\nFrom the regular update, “Involved Personnel” continues to \\nensure the priority is relevantContinue Monitor & Personalize\\n2024 Intel Flex Malaysia Engineer Ramp -up Program\\n•Introduction to Cloud Computing\\n•Containerization and Orchestration (Docker & Kubernetes)\\n•Front -end web development (Web development)\\n•Back -end development ( uServices  & API)FL II: Cloud Computing\\n•An Intro to Deep Learning and Convolutional Neural Networks\\n•DCGAN with Tensorflow\\n•OpenVINO  Inferencing and BenchmarkingFL  III: Data Science and Machine Learning\\n•Linux Kernel Development\\n•Introduction to ARM\\n•Introduction to BIOS & UEFI\\n•Introduction to Embedded Security\\n•DPCPP & oneAPI  OverviewFL IV: Embedded SystemsFocused List Overview: Focused List in Wiki\\n•1Source Introduction\\n•Fundamentals of Software Unified Process Lifecycle (Software @ Gladius)\\n•A Beginner’s Guide to Open Source Software Development\\n•Intro to Software Security\\n•Agile FoundationsFL I: Modern Software Development Practices', metadata={'page': 15, 'source': 'documents/2024 Intel Flex Engineering Malaysia - NEO_20240228.pdf'}),\n",
       " Document(page_content='New Employee Orientation (NEO)\\nQ1 2024Intel Flex Malaysia Engineering & TFM', metadata={'page': 0, 'source': 'documents/2024 Intel Flex Engineering Malaysia - NEO_20240228.pdf'}),\n",
       " Document(page_content='Intel Confidential Intel Flex\\n 17Involved Personnel\\n•“Project Lead ” + “ Mini -TFAL” + “ Technical Buddy ” work together to \\nidentify the prioritization for the new hire\\n•Who is Project Lead\\n•Flex engineer being the technical lead role in an engagement, even though not an official title from \\nengagement point -of-view, but has the necessary technical depth on the engagement\\n•Who is Mini -TFAL\\n•Technical Leader of each Mini -TFA within Intel Flex Engineering MY, incl. Cloud, Dascimal , Embedded, \\nTFM… and potentially more to come\\n•Who is Technical Buddy\\n•Someone helping the new hire during ramp -up, if assigned Buddy is from the same Mini -TFA, the buddy \\ncan play this role well\\n•Job -shadowed engineer is another candidate as well\\n•Note: These roles can be played by single person', metadata={'page': 16, 'source': 'documents/2024 Intel Flex Engineering Malaysia - NEO_20240228.pdf'}),\n",
       " Document(page_content='Intel Confidential Intel Flex\\n 18Ramp -Up Program Details\\n•Use P1, P2, P3 for priority levels; P1 being highest, and P3 lowest\\n•Under scenarios of conflicting priority, go for “ highest common denominator ” \\n, e.g. TFA vs. Project\\n•Any Corporate Mandatory Training is the highest priority when conflict between \\nsessions with internal ramp -up\\n•Work with to fine -tune\\n•p0: (specific training) < --dependencies to allow p1 & others to proceed.\\n•p1: project / investment areas\\n•p2: focused list\\n•p3: global list, TMT\\n•Recommend the new hire to send a weekly to FLM, everyone involved in \\nhelping the prioritization (Project Lead, Mini -TFAL and/or Technical Buddy)', metadata={'page': 17, 'source': 'documents/2024 Intel Flex Engineering Malaysia - NEO_20240228.pdf'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "retrieved_docs = retriever.invoke(\"Who are the new hires?\")\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    "    azure_deployment=\"llm-rag-chatgpt35\",\n",
    "    max_tokens=2048,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt template\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tan, An Nie\\nMohamad, Siti Nurhanisah (Hanisah)\\nTai, Andre\\nWei Xiang\\nEe, Elgene\\nDing RenNah, Wan Jun (Nicole)\\nTay, Xue Hao (Adam)\\nOng, Frankie\\nWei Quan\\n\\nThanks for asking!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"Who are the new hires\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.generate import *\n",
    "from rag.ingest import *\n",
    "from rag.retrieve import *\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingest\n",
      "load_documents took 130.0693 ms.\n",
      "chunk_documents took 0.6421 ms.\n",
      "save_chunks_to_chroma took 589.7884 ms.\n",
      "\n",
      "Retrieval\n",
      "retrieve took 29.3779 ms.\n",
      "\n",
      "Generation\n",
      "Generation took 2030.2331 ms. 34 words. 16.7468 words per second.\n",
      "\n",
      "Results:\n",
      " Query:\n",
      "\tWho are the new hires?\n",
      " Rag generation:\n",
      "\tThe new hires are Tan, An Nie; Mohamad, Siti Nurhanisah (Hanisah); Tai, Andre; Wei Xiang; Ee,\n",
      "Elgene; Ding RenNah, Wan Jun (Nicole); Tay, Xue Hao (Adam); Ong, Frankie; and Wei Quan. Thanks for\n",
      "asking!\n"
     ]
    }
   ],
   "source": [
    "# Query\n",
    "query = \"Who are the new hires?\"\n",
    "chromadb_save_path = \"database/chromadb/flex_neo\"\n",
    "\n",
    "# Ingest\n",
    "print(\"Ingest\")\n",
    "embedding_model = get_embedding_model(\"http://localhost:8188\")\n",
    "documents = load_documents(\"documents/2024 Intel Flex Engineering Malaysia - NEO_20240228.pdf\")\n",
    "chunks = chunk_documents(doc= documents, chunk_size=2000, chunk_overlap=0)\n",
    "save_chunks_to_chroma(chunks, embedding_model, chromadb_save_path)\n",
    "\n",
    "# Retriever\n",
    "print(\"\\nRetrieval\")\n",
    "retriever = get_chroma_retriever(chromadb_save_path, embedding_model)\n",
    "contexts = retrieve(retriever, query)\n",
    "\n",
    "# Rag Chain\n",
    "print(\"\\nGeneration\")\n",
    "llm = get_llm_azure(deployment_name = \"llm-rag-chatgpt35\", max_tokens=2048, temperature=0.7)\n",
    "prompt_template = get_prompt_template() # Use the default template by not giving input args\n",
    "rag_chain = create_rag_chain(retriever, prompt_template, llm)\n",
    "\n",
    "# Chat\n",
    "import time\n",
    "t1 = time.time()\n",
    "generated = rag_chain.invoke(query)\n",
    "t2 = time.time() -t1\n",
    "word_count = len(generated.split())\n",
    "print(f\"Generation took {(t2)*1000:.4f} ms. {word_count} words. {word_count / t2:.4f} words per second.\\n\")\n",
    "\n",
    "print(\"Results:\")\n",
    "print(f\" Query:\\n\\t{query}\")\n",
    "print(f\" Rag generation:\\n\\t{textwrap.fill(generated, width=100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response without RAG:\n",
      "\n",
      "Intel Flex Engineering is a program offered by Intel Corporation that allows employees to have a\n",
      "flexible work schedule and location. It enables employees to work from different locations,\n",
      "including their homes or other remote locations, as long as they meet their job requirements and\n",
      "deliverables. The program aims to provide employees with a better work-life balance and increased\n",
      "job satisfaction by allowing them to have more control over their work schedule and environment.\n",
      "\n",
      " Response with RAG:\n",
      "\n",
      "Intel Flex Engineering is a cross-platform software engineering team\n",
      "that focuses on innovative solutions and industry-leading\n",
      "technologies. They aim to improve developer effectiveness through\n",
      "automated CI/CD and analytics for faster deployments with better\n",
      "quality. Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "query = \"What does intel flex engineering do?\"\n",
    "print(\" Response without RAG:\\n\")\n",
    "print(textwrap.fill(llm.invoke(query).content, width=100))\n",
    "print()\n",
    "print(\" Response with RAG:\\n\")\n",
    "print(textwrap.fill(rag_chain.invoke(query)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral 7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.generate import *\n",
    "from rag.ingest import *\n",
    "from rag.retrieve import *\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingest\n",
      "load_documents took 254.5052 ms.\n",
      "chunk_documents took 0.5574 ms.\n",
      "ChromaDB already exists at the specified path. No changes made.\n",
      "save_chunks_to_chroma took 0.1526 ms.\n",
      "\n",
      "Retrieval\n",
      "retrieve took 33.6764 ms.\n",
      "\n",
      "Generation\n",
      "Generation took 75586.6714 ms. 30 words. 0.3969 words per second.\n",
      "\n",
      "Results:\n",
      " Query:\n",
      "\tWho are the new hires?\n",
      " Rag generation:\n",
      "\t The new hires are Tan An Nie, Mohamad Siti Nurhanisah (Hanisah), Tai Andre, Wei Xiang, Ee Elgene,\n",
      "Ding Ren, Nicole Wan Jun, Tay Xue Hao, Adam Ong Frankie, Wei Quan.\n"
     ]
    }
   ],
   "source": [
    "# Query\n",
    "query = \"Who are the new hires?\"\n",
    "chromadb_save_path = \"database/chromadb/flex_neo\"\n",
    "\n",
    "# Ingest\n",
    "print(\"Ingest\")\n",
    "embedding_model = get_embedding_model(\"http://localhost:8188\")\n",
    "documents = load_documents(\"documents/2024 Intel Flex Engineering Malaysia - NEO_20240228.pdf\")\n",
    "chunks = chunk_documents(doc= documents, chunk_size=2000, chunk_overlap=0)\n",
    "save_chunks_to_chroma(chunks, embedding_model, chromadb_save_path)\n",
    "\n",
    "# Retriever\n",
    "print(\"\\nRetrieval\")\n",
    "retriever = get_chroma_retriever(chromadb_save_path, embedding_model)\n",
    "contexts = retrieve(retriever, query)\n",
    "\n",
    "# Rag Chain \n",
    "print(\"\\nGeneration\")\n",
    "mistral_llm = get_llm_llamacpp(\"/data/llm/models/mistral-7b-instruct-v0.2.FP16.gguf\")\n",
    "prompt_template = get_prompt_template() # Use the default template by not giving input args\n",
    "rag_chain = create_rag_chain(retriever, prompt_template, mistral_llm)\n",
    "\n",
    "# Chat\n",
    "import time\n",
    "t1 = time.time()\n",
    "generated = rag_chain.invoke(query)\n",
    "t2 = time.time() -t1\n",
    "word_count = len(generated.split())\n",
    "print(f\"Generation took {(t2)*1000:.4f} ms. {word_count} words. {word_count / t2:.4f} words per second.\\n\")\n",
    "\n",
    "print(\"Results:\")\n",
    "print(f\" Query:\\n\\t{query}\")\n",
    "print(f\" Rag generation:\\n\\t{textwrap.fill(generated, width=100)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computex",
   "language": "python",
   "name": "computex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
