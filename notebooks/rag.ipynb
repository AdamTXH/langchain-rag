{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to root directory\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest into vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model=thenlper/gte-large\n",
    "volume=/data/llm/hf-tei-data\n",
    "docker images run --gpus all --env HTTPS_PROXY=$https_proxy --env HTTP_PROXY=$http_proxy -p 8188:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:turing-0.6 --model-id $model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceHubEmbeddings\n",
    "embeddings_model = HuggingFaceHubEmbeddings(model=\"http://localhost:8188\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, chunk, store source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"documents/2024 Intel Flex Engineering Malaysia - NEO_20240228.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(pages)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings_model, persist_directory=\"database/chromadb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vecstore = Chroma(persist_directory=\"database/chromadb\", embedding_function=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Intel Confidential Intel Flex\\n 2Intel Flex Malaysia Engineering & TFM\\nRecent New Hires\\nEngineering TFM\\nTan, An Nie\\nMohamad, Siti Nurhanisah\\n(Hanisah)\\nTai, Andre\\nWei Xiang\\nEe, Elgene  \\nDing RenNah, Wan Jun\\n(Nicole )\\nTay, Xue Hao\\n(Adam )Ong, Frankie\\nWei Quan', metadata={'page': 1, 'source': 'documents/2024 Intel Flex Engineering Malaysia - NEO_20240228.pdf'}),\n",
       " Document(page_content='Intel Confidential Intel Flex\\n 13Experienced employee assigned to \\nbefriend the new hire in order to help \\nintegrating the new hire to Intel and \\nIntel Flex.\\nHello, buddy!\\nQuarterly Group Buddy Meetup1:1 Meeting\\n6 months\\nBuddy Check List\\n Buddy Lunch\\nLearn More: Intel Flex Malaysia - New Hire Integration Program', metadata={'page': 12, 'source': 'documents/2024 Intel Flex Engineering Malaysia - NEO_20240228.pdf'}),\n",
       " Document(page_content='Intel Confidential Intel Flex\\n 16\\nEach new hire will receive multiple ramp -up list, incl. Focused List \\nfrom Mini -TFA, Modern SW, potentially project -specific and Flex \\nGlobal passdownMultiple Ramp -Up Lists (Intensive Academy, Mini -TFA, MSW…)Buddy Program initiatedNew Hire Joins\\n1. Project Lead, Mini -TFAL and Technical Buddy work together \\nto prioritize for the resource\\n2. Identify the completion date of the ramp -upRamp -Up Personalization with “Involved Personnel”\\n1. Project specific, skills ramp -up & task execution likely go hand -\\nin-hand together.\\n2. Mini -TFAL provides the fundamental exposure to the field \\nbeyond “project -specific”Skills Practice\\nRecommended to have weekly progress with 2 parts, project \\nspecific tasks (if projects assigned) and personalized list from step \\n#2Regular Update\\nFrom the regular update, “Involved Personnel” continues to \\nensure the priority is relevantContinue Monitor & Personalize\\n2024 Intel Flex Malaysia Engineer Ramp -up Program\\n•Introduction to Cloud Computing\\n•Containerization and Orchestration (Docker & Kubernetes)\\n•Front -end web development (Web development)\\n•Back -end development ( uServices  & API)FL II: Cloud Computing\\n•An Intro to Deep Learning and Convolutional Neural Networks\\n•DCGAN with Tensorflow\\n•OpenVINO  Inferencing and BenchmarkingFL  III: Data Science and Machine Learning\\n•Linux Kernel Development\\n•Introduction to ARM\\n•Introduction to BIOS & UEFI\\n•Introduction to Embedded Security\\n•DPCPP & oneAPI  OverviewFL IV: Embedded SystemsFocused List Overview: Focused List in Wiki\\n•1Source Introduction\\n•Fundamentals of Software Unified Process Lifecycle (Software @ Gladius)\\n•A Beginner’s Guide to Open Source Software Development\\n•Intro to Software Security\\n•Agile FoundationsFL I: Modern Software Development Practices', metadata={'page': 15, 'source': 'documents/2024 Intel Flex Engineering Malaysia - NEO_20240228.pdf'}),\n",
       " Document(page_content='New Employee Orientation (NEO)\\nQ1 2024Intel Flex Malaysia Engineering & TFM', metadata={'page': 0, 'source': 'documents/2024 Intel Flex Engineering Malaysia - NEO_20240228.pdf'}),\n",
       " Document(page_content='Intel Confidential Intel Flex\\n 17Involved Personnel\\n•“Project Lead ” + “ Mini -TFAL” + “ Technical Buddy ” work together to \\nidentify the prioritization for the new hire\\n•Who is Project Lead\\n•Flex engineer being the technical lead role in an engagement, even though not an official title from \\nengagement point -of-view, but has the necessary technical depth on the engagement\\n•Who is Mini -TFAL\\n•Technical Leader of each Mini -TFA within Intel Flex Engineering MY, incl. Cloud, Dascimal , Embedded, \\nTFM… and potentially more to come\\n•Who is Technical Buddy\\n•Someone helping the new hire during ramp -up, if assigned Buddy is from the same Mini -TFA, the buddy \\ncan play this role well\\n•Job -shadowed engineer is another candidate as well\\n•Note: These roles can be played by single person', metadata={'page': 16, 'source': 'documents/2024 Intel Flex Engineering Malaysia - NEO_20240228.pdf'}),\n",
       " Document(page_content='Intel Confidential Intel Flex\\n 18Ramp -Up Program Details\\n•Use P1, P2, P3 for priority levels; P1 being highest, and P3 lowest\\n•Under scenarios of conflicting priority, go for “ highest common denominator ” \\n, e.g. TFA vs. Project\\n•Any Corporate Mandatory Training is the highest priority when conflict between \\nsessions with internal ramp -up\\n•Work with to fine -tune\\n•p0: (specific training) < --dependencies to allow p1 & others to proceed.\\n•p1: project / investment areas\\n•p2: focused list\\n•p3: global list, TMT\\n•Recommend the new hire to send a weekly to FLM, everyone involved in \\nhelping the prioritization (Project Lead, Mini -TFAL and/or Technical Buddy)', metadata={'page': 17, 'source': 'documents/2024 Intel Flex Engineering Malaysia - NEO_20240228.pdf'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "retrieved_docs = retriever.invoke(\"Who are the new hires?\")\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    "    azure_deployment=\"llm-rag-chatgpt35\",\n",
    "    max_tokens=2048,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt template\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tan, An Nie\\nMohamad, Siti Nurhanisah (Hanisah)\\nTai, Andre\\nWei Xiang\\nEe, Elgene\\nDing RenNah, Wan Jun (Nicole)\\nTay, Xue Hao (Adam)\\nOng, Frankie\\nWei Quan\\n\\nThanks for asking!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"Who are the new hires\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "from rag.generate import *\n",
    "from rag.ingest import *\n",
    "from rag.retrieve import *\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/adamtay/computex'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingest\n",
      "load_documents took 131.2447 ms.\n",
      "chunk_documents took 0.7067 ms.\n",
      "ChromaDB already exists at the specified path. No changes made.\n",
      "save_chunks_to_chroma took 0.0196 ms.\n",
      "\n",
      "Retrieval\n",
      "retrieve took 129.9841 ms.\n",
      "\n",
      "Generation\n",
      "Generation took 2019.6245 ms. 33 words. 16.3397 words per second.\n",
      "\n",
      "Results:\n",
      " Query:\n",
      "\tWho are the new hires?\n",
      " Rag generation:\n",
      "\tTan, An Nie, Mohamad, Siti Nurhanisah (Hanisah), Tai, Andre, Wei Xiang, Ee, Elgene, Ding RenNah, Wan\n",
      "Jun (Nicole), Tay, Xue Hao (Adam), Ong, Frankie, Wei Quan are the new hires.  Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "# Query\n",
    "query = \"Who are the new hires?\"\n",
    "chromadb_save_path = \"database/chromadb/flex_neo\"\n",
    "\n",
    "# Ingest\n",
    "print(\"Ingest\")\n",
    "embedding_model = get_embedding_model(\"http://localhost:8188\")\n",
    "documents = load_documents(\"documents/2024 Intel Flex Engineering Malaysia - NEO_20240228.pdf\")\n",
    "chunks = chunk_documents(doc= documents, chunk_size=2000, chunk_overlap=0)\n",
    "save_chunks_to_chroma(chunks, embedding_model, chromadb_save_path)\n",
    "\n",
    "# Retriever\n",
    "print(\"\\nRetrieval\")\n",
    "retriever = get_chroma_retriever(chromadb_save_path, embedding_model)\n",
    "contexts = retrieve(retriever, query)\n",
    "\n",
    "# Rag Chain\n",
    "print(\"\\nGeneration\")\n",
    "llm = get_llm_azure(deployment_name = \"llm-rag-chatgpt35\", max_tokens=2048, temperature=0.7)\n",
    "prompt_template = get_prompt_template() # Use the default template by not giving input args\n",
    "rag_chain = create_rag_chain(retriever, prompt_template, llm)\n",
    "\n",
    "# Chat\n",
    "import time\n",
    "t1 = time.time()\n",
    "generated = rag_chain.invoke(query)\n",
    "t2 = time.time() -t1\n",
    "word_count = len(generated.split())\n",
    "print(f\"Generation took {(t2)*1000:.4f} ms. {word_count} words. {word_count / t2:.4f} words per second.\\n\")\n",
    "\n",
    "print(\"Results:\")\n",
    "print(f\" Query:\\n\\t{query}\")\n",
    "print(f\" Rag generation:\\n\\t{textwrap.fill(generated, width=100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new hires are Tan, An Nie; Mohamad, Siti Nurhanisah (Hanisah); Tai, Andre; Wei Xiang; Ee, Elgene; Ding RenNah, Wan Jun (Nicole); Tay, Xue Hao (Adam); and Ong, Frankie Wei Quan. Thanks for asking!"
     ]
    }
   ],
   "source": [
    "# Streaming\n",
    "async for chunk in rag_chain.astream(query):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new hires are Tan, An Nie; Mohamad, Siti Nurhanisah (Hanisah); Tai, Andre; Wei Xiang; Ee, Elgene; Ding RenNah, Wan Jun (Nicole); Tay, Xue Hao (Adam); and Ong, Frankie Wei Quan. Thanks for asking!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The new hires are Tan, An Nie; Mohamad, Siti Nurhanisah (Hanisah); Tai, Andre; Wei Xiang; Ee, Elgene; Ding RenNah, Wan Jun (Nicole); Tay, Xue Hao (Adam); and Ong, Frankie Wei Quan. Thanks for asking!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await stream_output(rag_chain, query)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response without RAG:\n",
      "\n",
      "Intel Flex Engineering is a program offered by Intel Corporation that allows employees to have a\n",
      "flexible work schedule and location. It enables employees to work from different locations,\n",
      "including their homes or other remote locations, as long as they meet their job requirements and\n",
      "deliverables. The program aims to provide employees with a better work-life balance and increased\n",
      "job satisfaction by allowing them to have more control over their work schedule and environment.\n",
      "\n",
      " Response with RAG:\n",
      "\n",
      "Intel Flex Engineering is a cross-platform software engineering team\n",
      "that focuses on innovative solutions and industry-leading\n",
      "technologies. They aim to improve developer effectiveness through\n",
      "automated CI/CD and analytics for faster deployments with better\n",
      "quality. Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "query = \"What does intel flex engineering do?\"\n",
    "print(\" Response without RAG:\\n\")\n",
    "print(textwrap.fill(llm.invoke(query).content, width=100))\n",
    "print()\n",
    "print(\" Response with RAG:\\n\")\n",
    "print(textwrap.fill(rag_chain.invoke(query)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral 7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.generate import *\n",
    "from rag.ingest import *\n",
    "from rag.retrieve import *\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/adamtay/computex/notebookes'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingest\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "File path ./documents/2024 Intel Flex Engineering Malaysia - NEO_20240228.pdf is not a valid file or url",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIngest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m get_embedding_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:8188\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m documents \u001b[38;5;241m=\u001b[39m load_documents(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./documents/2024 Intel Flex Engineering Malaysia - NEO_20240228.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m chunks \u001b[38;5;241m=\u001b[39m chunk_documents(doc\u001b[38;5;241m=\u001b[39m documents, chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     10\u001b[0m save_chunks_to_chroma(chunks, embedding_model, chromadb_save_path)\n",
      "File \u001b[0;32m~/computex/notebookes/../rag/utils.py:7\u001b[0m, in \u001b[0;36mbenchmark.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m      6\u001b[0m     t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 7\u001b[0m     result \u001b[38;5;241m=\u001b[39m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m      8\u001b[0m     t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(t2\u001b[38;5;241m-\u001b[39mt1)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1000\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ms.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/computex/notebookes/../rag/ingest.py:16\u001b[0m, in \u001b[0;36mload_documents\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;129m@benchmark\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_documents\u001b[39m(file_path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 16\u001b[0m     loader \u001b[38;5;241m=\u001b[39m PyPDFLoader(file_path)\n\u001b[1;32m     17\u001b[0m     doc \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[0;32m~/miniconda3/envs/computex/lib/python3.11/site-packages/langchain_community/document_loaders/pdf.py:182\u001b[0m, in \u001b[0;36mPyPDFLoader.__init__\u001b[0;34m(self, file_path, password, headers, extract_images)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpypdf package not found, please install it with \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install pypdf`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    181\u001b[0m     )\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(file_path, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser \u001b[38;5;241m=\u001b[39m PyPDFParser(password\u001b[38;5;241m=\u001b[39mpassword, extract_images\u001b[38;5;241m=\u001b[39mextract_images)\n",
      "File \u001b[0;32m~/miniconda3/envs/computex/lib/python3.11/site-packages/langchain_community/document_loaders/pdf.py:116\u001b[0m, in \u001b[0;36mBasePDFLoader.__init__\u001b[0;34m(self, file_path, headers)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(temp_pdf)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path):\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile path \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not a valid file or url\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)\n",
      "\u001b[0;31mValueError\u001b[0m: File path ./documents/2024 Intel Flex Engineering Malaysia - NEO_20240228.pdf is not a valid file or url"
     ]
    }
   ],
   "source": [
    "# Query\n",
    "query = \"Who are the new hires?\"\n",
    "chromadb_save_path = \"database/chromadb/flex_neo\"\n",
    "\n",
    "# Ingest\n",
    "print(\"Ingest\")\n",
    "embedding_model = get_embedding_model(\"http://localhost:8188\")\n",
    "documents = load_documents(\"./documents/2024 Intel Flex Engineering Malaysia - NEO_20240228.pdf\")\n",
    "chunks = chunk_documents(doc= documents, chunk_size=2000, chunk_overlap=0)\n",
    "save_chunks_to_chroma(chunks, embedding_model, chromadb_save_path)\n",
    "\n",
    "# Retriever\n",
    "print(\"\\nRetrieval\")\n",
    "retriever = get_chroma_retriever(chromadb_save_path, embedding_model)\n",
    "contexts = retrieve(retriever, query)\n",
    "\n",
    "# Rag Chain \n",
    "print(\"\\nGeneration\")\n",
    "mistral_llm = get_llm_llamacpp(\"/data/llm/models/mistral-7b-instruct-v0.2.FP16.gguf\")\n",
    "prompt_template = get_prompt_template() # Use the default template by not giving input args\n",
    "rag_chain = create_rag_chain(retriever, prompt_template, mistral_llm)\n",
    "\n",
    "# Chat\n",
    "import time\n",
    "t1 = time.time()\n",
    "generated = rag_chain.invoke(query)\n",
    "t2 = time.time() -t1\n",
    "word_count = len(generated.split())\n",
    "print(f\"Generation took {(t2)*1000:.4f} ms. {word_count} words. {word_count / t2:.4f} words per second.\\n\")\n",
    "\n",
    "print(\"Results:\")\n",
    "print(f\" Query:\\n\\t{query}\")\n",
    "print(f\" Rag generation:\\n\\t{textwrap.fill(generated, width=100)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computex",
   "language": "python",
   "name": "computex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
